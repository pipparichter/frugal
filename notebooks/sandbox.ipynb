{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.tools import MMseqs, NCBIDatasets, UniRef\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.files import XMLFile, InterProScanFile, FASTAFile, GBFFFile\n",
    "from utils import * \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from src.classifier import Classifier\n",
    "from src.sampler import Sampler\n",
    "from src.genome import ReferenceGenome\n",
    "from src.dataset import Dataset\n",
    "import re\n",
    "from src.build import * \n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phylum count: 56\n",
      "Class count: 123\n",
      "Order count: 299\n",
      "Genus count: 3983\n"
     ]
    }
   ],
   "source": [
    "ncbi_genome_metadata_df = load_ncbi_genome_metadata()\n",
    "\n",
    "for level in ['phylum', 'class', 'order', 'genus']:\n",
    "    print(f'{level.capitalize()} count:', ncbi_genome_metadata_df[f'{level}'].nunique())\n",
    "    # print(f'Species with no {level}:', (genome_metadata_df[f'{level}_taxid'] == 'none').sum())\n",
    "\n",
    "# fig, (ax_a, ax_b) = plt.subplots(ncols=2, figsize=(15, 5), width_ratios=[0.3, 0.7])\n",
    "\n",
    "# counts, bins, _ = ax_a.hist(ncbi_genome_metadata_df.gc_percent, bins=25, width=2, edgecolor='black', color='lightgray')\n",
    "# ax_a.set_ylabel('density')\n",
    "# ax_a.set_xlabel('GC percent')\n",
    "\n",
    "# level = 'phylum'\n",
    "# # x = np.arange(genome_metadata_df[f'{level}_taxid'].nunique())\n",
    "# y = ncbi_genome_metadata_df.groupby(level).gc_percent.mean() # .sort_values(ascending=True)\n",
    "# x = ncbi_genome_metadata_df.groupby(level).apply(len, include_groups=False) # .sort_values(ascending=True)\n",
    "# y_err = ncbi_genome_metadata_df.groupby(level).apply(lambda df : df.gc_percent.std() / np.sqrt(len(df)), include_groups=False) # .loc[y.index]\n",
    "\n",
    "# ax_b.errorbar(x, y, yerr=y_err, ls='', lw=0.7, capsize=2, color='black')\n",
    "# ax_b.scatter(x, y, color='black', s=5)\n",
    "# ax_b.set_xscale('log')\n",
    "# ax_b.set_xlabel(f'log({level} size)')\n",
    "# ax_b.set_ylabel('GC percent')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-GC content phylum: Campylobacterota\n",
      "Medium-GC content phylum: Planctomycetota\n",
      "High-GC content phylum: Deinococcota\n"
     ]
    }
   ],
   "source": [
    "# How should I select the genomes for model training?\n",
    "\n",
    "# Using the same genomes for validation as in the Prodigal paper\n",
    "# Halobacterium salinarum\n",
    "# Natronomonas pharaonis\n",
    "# Aeropyrum pernix\n",
    "model_organism_genome_ids = ['GCF_000005845.2'] # E. coli K-12\n",
    "model_organism_genome_ids += ['GCF_000009045.1'] # B. subtilis\n",
    "model_organism_genome_ids += ['GCF_000006765.1'] # P. aeruginosa\n",
    "\n",
    "# Start by selecting a few phyla in a reasonable size range with varying GC contents.\n",
    "def sample(genome_metadata_df:pd.DataFrame, min_phylum_size:int=90, max_phylum_size:int=200):\n",
    "\n",
    "    phylum_sizes = genome_metadata_df.groupby('phylum').apply(len, include_groups=False)\n",
    "    phylum_sizes = phylum_sizes[(phylum_sizes > min_phylum_size) & (phylum_sizes < max_phylum_size)]\n",
    "\n",
    "    genome_metadata_df = genome_metadata_df[genome_metadata_df.phylum.isin(phylum_sizes.index)]\n",
    "    \n",
    "    phylum_gc_percent = genome_metadata_df.groupby('phylum').gc_percent.mean() \n",
    "    phylum_gc_percent = phylum_gc_percent.sort_values(ascending=True)\n",
    "    \n",
    "    phyla = dict()\n",
    "    phyla['low_gc_content'] = phylum_gc_percent.index[0]\n",
    "    phyla['med_gc_content'] = phylum_gc_percent.index[len(phylum_gc_percent) // 2]\n",
    "    phyla['high_gc_content'] = phylum_gc_percent.index[-1]\n",
    "\n",
    "    print('Low-GC content phylum:', phyla['low_gc_content'])\n",
    "    print('Medium-GC content phylum:', phyla['med_gc_content'])\n",
    "    print('High-GC content phylum:', phyla['high_gc_content'])\n",
    "\n",
    "    genome_ids = {category:genome_metadata_df.index[genome_metadata_df.phylum == phylum] for category, phylum in phyla.items()}\n",
    "    return genome_ids\n",
    "\n",
    "genome_ids = sample(ncbi_genome_metadata_df)\n",
    "\n",
    "# ncbi = NCBIDatasets(genome_dir='../data/genomes', gbff_dir='../data/proteins/ncbi')\n",
    "# ncbi.run(genome_ids=genome_ids['low_gc_content'], metadata_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_metadata_df = list()\n",
    "for category, genome_ids_ in genome_ids.items():\n",
    "    df = ncbi_genome_metadata_df.loc[genome_ids_, :].copy()\n",
    "    df['gc_percent_category'] = category\n",
    "    # df = df[genome_metadata_df.annotation_pipeline.str.contains('PGAP')] # Only include genomes annotated with PGAP, because the other files are not consistent. \n",
    "    genome_metadata_df.append(df)\n",
    "genome_metadata_df = pd.concat(genome_metadata_df)\n",
    "# genome_metadata_df.to_csv('../data/genome_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.tools import download_homologs\n",
    "# for path in glob.glob('../data/proteins/ncbi/*'):\n",
    "#     download_homologs(path, pseudo_only=True, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genome_id in genome_ids['med_gc_content'][::-1]:\n",
    "    ! ref --input-path ../data/proteins/prodigal/{genome_id}_protein.faa --output-dir ../data/ref --reference-dir ../data/proteins/ncbi/ --prodigal-output --summarize --load-homologs --homologs-dir ../data/proteins/homologs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = load_ref(genome_ids=[genome_id for genome_id in genome_metadata_df.index if (genome_id != 'GCF_000009085.1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_spurious(ref_df:pd.DataFrame, max_score:float=0.5, remove_invalid:bool=True):\n",
    "    scores = get_alignment_scores(ref_df, seq_a_col='top_hit_seq', seq_b_col='query_seq')\n",
    "    # ref_df['alignment_score'] = scores\n",
    "\n",
    "    valid = ref_df.top_hit_feature.isin(GBFFFile.rna_features + GBFFFile.other_features)\n",
    "    valid = valid | ((scores < max_score) & ~np.isnan(scores)) \n",
    "    ref_df['valid'] = valid\n",
    "\n",
    "    print(f'check_spurious: Validated {ref_df.valid.sum()} out of {len(ref_df)} spurious sequences.')\n",
    "\n",
    "    if remove_invalid:\n",
    "        ref_df = ref_df[ref_df.valid].copy()\n",
    "        ref_df = ref_df.drop(columns=['valid'])\n",
    "    return ref_df\n",
    "    \n",
    "\n",
    "def check_real(ref_df:pd.DataFrame, min_score:float=0.9, remove_invalid:bool=True):\n",
    "    scores = get_alignment_scores(ref_df, seq_a_col='top_hit_seq', seq_b_col='query_seq')\n",
    "    # ref_df['alignment_score'] = scores\n",
    "    ref_df['valid'] = (scores > min_score) & ~np.isnan(scores)\n",
    "\n",
    "    print(f'check_real: Validated {ref_df.valid.sum()} out of {len(ref_df)} real sequences.')\n",
    "    \n",
    "    if remove_invalid:\n",
    "        ref_df = ref_df[ref_df.valid].copy()\n",
    "        ref_df = ref_df.drop(columns=['valid'])\n",
    "    return ref_df\n",
    "\n",
    "# It seems that only including things with definitive labels seems to have just increased the number of false negatives. \n",
    "# Possibly the benefits of increasing label confidence were outweighed by the shrinkage of the minority class?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(genome_metadata_df:pd.DataFrame, phylum:str='Campylobacterota', max_length:int=2000):\n",
    "    # First remove things which do not have definitive labels. \n",
    "    genome_ids = genome_metadata_df[genome_metadata_df.phylum == phylum].index\n",
    "    genome_ids = [genome_id for genome_id in genome_metadata_df.index if (genome_id != 'GCF_000009085.1')] # Remove the non-PGAP genome. \n",
    "\n",
    "    print(f'build_datasets: Loading data from {len(genome_ids)} genomes belonging to the phylum {phylum}.')\n",
    "    df = load_ref(genome_ids=genome_ids)\n",
    "    print(f'build_datasets: Loaded {len(ref_df)} sequences.')\n",
    "\n",
    "    df = pd.concat([check_real(df[df.real].copy()), check_spurious(df[df.spurious].copy())])\n",
    "    print(f'build_datasets: Kept {df.spurious.sum()} spurious sequences and {df.real.sum()} real sequences.')\n",
    "    df['label'] = df.real.astype(int)\n",
    "\n",
    "    lengths = get_lengths(df, top_hit=False)\n",
    "    print(f'Removing {(lengths >= max_length).sum()} sequences exceeding the maximum length of {max_length}')\n",
    "    df = df[lengths < max_length]\n",
    "\n",
    "    df = df.rename(columns={'query_seq':'seq'}) # Need to do this for file writing, etc. to work correctly, \n",
    "    df = df.rename(columns={'top_hit_genome_id':'genome_id'}) # Need to do this for file writing, etc. to work correctly, \n",
    "\n",
    "    mmseqs = MMseqs()\n",
    "    df = mmseqs.cluster(df, job_name=f'{phylum.lower()}_95', sequence_identity=0.95, reps_only=True, overwrite=True)\n",
    "    mmseqs.cleanup()\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idxs, test_idxs = list(gss.split(df, groups=df.genome_id))[0]\n",
    "    train_df, test_df = df.iloc[train_idxs], df.iloc[test_idxs]\n",
    "    print(f'build_datasets: Training dataset contains {len(train_df)} sequences, testing dataset contains {len(test_df)} sequences.')\n",
    "    print(f'build_datasets: {(train_df.label == 0).sum()} negative instances and {(train_df.label == 1).sum()} positive instances in the training dataset.')\n",
    "    print(f'build_datasets: {(test_df.label == 0).sum()} negative instances and {(test_df.label == 1).sum()} positive instances in the testing dataset.')\n",
    "    train_df.to_csv(f'../data/{phylum.lower()}_dataset_train.csv')\n",
    "    test_df.to_csv(f'../data/{phylum.lower()}_dataset_test.csv')\n",
    "    return train_df, test_df \n",
    "    \n",
    "# train_df, test_df = build_datasets(genome_metadata_df)\n",
    "phylum ='Campylobacterota'\n",
    "dtypes = {'top_hit_partial':str, 'query_partial':str, 'top_hit_translation_table':str, 'top_hit_codon_start':str}\n",
    "train_df = pd.read_csv(f'../data/{phylum.lower()}_dataset_train.csv', index_col=0, dtype=dtypes)\n",
    "test_df = pd.read_csv(f'../data/{phylum.lower()}_dataset_test.csv', index_col=0, dtype=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = load_predict('../data/predict/campylobacterota_dataset_test_predict.csv', model_name='campylobacterota_esm_650m_gap_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.8243785650516682\n",
      "Precision (class 0): 0.0003928135789451922\n",
      "Recall (class 0): 0.12666666666666668\n",
      "Number of false negatives: 14003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NC_017735.1_1005', 'NC_017735.1_1006', 'NC_017735.1_1007', ...,\n",
       "       'NZ_WFKI01000156.1_1', 'NZ_WFKI01000159.1_1',\n",
       "       'NZ_WFKI01000162.1_1'], shape=(2067,), dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class BenchmarkClassifier():\n",
    "\n",
    "    def __init__(self, feature_col:str='query_length'):\n",
    "\n",
    "        self.model = LogisticRegression(class_weight='balanced', C=1) # , multi_class='multinomial') \n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "    def predict(self, df:pd.DataFrame, include_outputs:bool=True):\n",
    "        X = df[[self.feature_col]].values\n",
    "        X = self.scaler.transform(X)\n",
    "        model_labels = self.model.predict(X)\n",
    "        model_outputs = self.model.predict_proba(X)\n",
    "\n",
    "        return model_labels if (not include_outputs) else (model_labels, model_outputs)\n",
    "\n",
    "    def fit(self, df:pd.DataFrame):\n",
    "        X, labels = df[[self.feature_col]].values, df['label'].values\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, labels)\n",
    "\n",
    "model = BenchmarkClassifier()\n",
    "model.fit(train_df)\n",
    "\n",
    "df = test_df[['label', 'query_length']].copy()\n",
    "df['model_label'], outputs = model.predict(test_df, include_outputs=True)\n",
    "df['model_output_0'], df['model_output_1'] = outputs[:, 0].ravel(), outputs[:, 1].ravel()\n",
    "\n",
    "print('Balanced accuracy:', balanced_accuracy_score(df.label.values, df.model_label.values))\n",
    "print('Precision (class 0):', precision(df, class_=0))\n",
    "print('Recall (class 0):', recall(df, class_=0))\n",
    "print('Number of false negatives:', ((df.label == 1) & (df.model_label == 0)).sum())\n",
    "\n",
    "\n",
    "confusion_matrix = np.where((df.model_label == 1) & (df.label == 0), 'false positive', '')\n",
    "confusion_matrix = np.where((df.model_label  == 1) & (df.label == 1), 'true positive', confusion_matrix)\n",
    "confusion_matrix = np.where((df.model_label == 0) & (df.label == 1), 'false negative', confusion_matrix)\n",
    "confusion_matrix = np.where((df.model_label  == 0) & (df.label == 0), 'true negative', confusion_matrix)\n",
    "df['confusion_matrix'] = confusion_matrix\n",
    "\n",
    "benchmark_fn = df[df.confusion_matrix == 'false negative'].index\n",
    "fn = pred_df[pred_df.confusion_matrix == 'false negative'].index\n",
    "np.intersect1d(fn, benchmark_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There still seems to be something wrong with the training data, perhaps I am still misclassifying the spurious sequences?\n",
    "# It is possible my \"in-frame\" check is wrong, it's possible I should only see if either edge is a multiple of 3 base pairs away, \n",
    "# as opposed to both.\n",
    "\n",
    "# It seems as though 684 of the sequences flagged as spurious have an overlap length of 0, so maybe I should be counting those as intergenic?\n",
    "# Ok, so I think because of how frequent gene overlap is, I should be allowing some (considering genes with less than a certain amount of overlap to be intergenic)\n",
    "# I should also be very conservative, i.e. allow a comparatively large amount of overlap. \n",
    "# https://pmc.ncbi.nlm.nih.gov/articles/PMC525685/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Curious about the distribution of overlap sizes... \n",
    "\n",
    "\n",
    "# def figure(ref_df:pd.DataFrame, path:str=None, min_overlap_fraction:float=0.8):\n",
    "#     # ref_df = load_ref(genome_ids=genome_ids)\n",
    "#     ref_df = ref_df[ref_df.spurious | ref_df.real].copy()\n",
    "\n",
    "#     fig = plt.figure(figsize=(10, 10), layout='tight')\n",
    "#     grid = GridSpec(nrows=1, ncols=1, figure=fig)\n",
    "#     ax_a = fig.add_subplot(grid[0, 0])\n",
    "\n",
    "#     ax_a_df = pd.DataFrame(index=ref_df.index)\n",
    "#     ax_a_df['length'] = get_lengths(ref_df, top_hit=False, units='nt')\n",
    "#     ax_a_df['overlap_length'] = ref_df['top_hit_overlap_length']\n",
    "#     ax_a_df['overlap_fraction'] = ax_a_df.overlap_length / ax_a_df.length # This is relative to the query sequence. \n",
    "#     ax_a_df['label'] = np.select([ref_df.real, ref_df.spurious], ['real', 'spurious'], default='none')\n",
    "#     if min_overlap_fraction is not None:\n",
    "#         ax_a_df = ax_a_df[ax_a_df.overlap_fraction > min_overlap_fraction].copy()\n",
    "\n",
    "#     colors = ['darkseagreen', 'indianred']\n",
    "#     # sns.kdeplot(data=ax_a_df, ax=ax_a, hue='label', hue_order=['real', 'spurious'], palette=colors, common_norm=False, x='overlap_fraction')\n",
    "#     sns.ecdfplot(data=ax_a_df, ax=ax_a, hue='label', hue_order=['real', 'spurious'], palette=colors, x='overlap_fraction')\n",
    "\n",
    "#     plt.show()\n",
    "#     return ax_a_df\n",
    "\n",
    "\n",
    "# ax_a_df = figure(ref_df, min_overlap_fraction=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tripy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
