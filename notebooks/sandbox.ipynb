{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import MMseqs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.files import XMLFile, InterProScanFile, FASTAFile\n",
    "from utils import * \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_filter_datasets: Loaded 464706 proteins from ./ref.out\n",
      "remove_partial: Removing 4309 sequences marked as partial by both Prodigal and the reference.\n",
      "get_filter_datasets: Removing 16946 proteins which exceed the maximum specified length of 800.\n",
      "get_filter_datasets: 42823 out of 443451 proteins labeled as Prodigal errors.\n",
      "get_filter_datasets Removing 0 spurious NCBI proteins from the set of \"negative\" test instances.\n",
      "MMseqs.load: Removing 3210 non-cluster representatives.\n",
      "get_filter_datasets: Training dataset contains 352192 proteins, testing dataset contains 88049 proteins.\n"
     ]
    }
   ],
   "source": [
    "def get_filter_datasets(max_length:int=800):\n",
    "\n",
    "    df = load_ref_out()\n",
    "    print(f'get_filter_datasets: Loaded {len(df)} proteins from ./ref.out')\n",
    "    df = remove_partial(df)\n",
    "\n",
    "    interpro_df = InterProScanFile('../data/putative_protein.interpro.tsv').to_df(max_e_value=1e-5, drop_duplicates=True)\n",
    "    interpro_df.columns = ['interpro_' + col for col in interpro_df.columns]\n",
    "    df = df.merge(interpro_df, left_index=True, right_index=True, how='left', validate='one_to_one')\n",
    "\n",
    "    df['length'] = df.seq.apply(len)\n",
    "    mask = df.length > max_length\n",
    "    print(f'get_filter_datasets: Removing {mask.sum()} proteins which exceed the maximum specified length of {max_length}.')\n",
    "    df = df[~mask].copy()\n",
    "\n",
    "    df['label'] = df.apply(is_prodigal_error, axis=1).astype(int)\n",
    "    print(f'get_filter_datasets: {df.label.sum()} out of {len(df)} proteins labeled as Prodigal errors.')\n",
    "\n",
    "    mask = df.apply(is_ncbi_error, axis=1) & (df.label == 0)\n",
    "    print(f'get_filter_datasets Removing {mask.sum()} spurious NCBI proteins from the set of \"negative\" test instances.')\n",
    "    df = df[~mask]\n",
    "\n",
    "    mmseqs = MMseqs()\n",
    "    df = mmseqs.cluster(df, job_name='filter', sequence_identity=0.95, reps_only=True, overwrite=True)\n",
    "    mmseqs.cleanup()\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    print(f'get_filter_datasets: Training dataset contains {len(train_df)} proteins, testing dataset contains {len(test_df)} proteins.')\n",
    "    train_df.to_csv('../data/filter_dataset_train.csv')\n",
    "    test_df.to_csv('../data/filter_dataset_test.csv')\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = get_filter_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tripy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
