{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.tools import MMseqs, NCBIDatasets, UniRef\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.files import XMLFile, InterProScanFile, FASTAFile, GBFFFile\n",
    "from utils import * \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from src.classifier import Classifier\n",
    "from src.sampler import Sampler\n",
    "from src.genome import ReferenceGenome\n",
    "from src.dataset import Dataset\n",
    "import re\n",
    "from src.build import * \n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier.load('../models/campylobacterota_esm_650m_gap_v1.pkl')\n",
    " \n",
    "model.sampler.balance_classes\n",
    "model.sampler.balance_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phylum count: 56\n",
      "Class count: 123\n",
      "Order count: 299\n",
      "Genus count: 3983\n"
     ]
    }
   ],
   "source": [
    "ncbi_genome_metadata_df = load_ncbi_genome_metadata()\n",
    "\n",
    "for level in ['phylum', 'class', 'order', 'genus']:\n",
    "    print(f'{level.capitalize()} count:', ncbi_genome_metadata_df[f'{level}'].nunique())\n",
    "    # print(f'Species with no {level}:', (genome_metadata_df[f'{level}_taxid'] == 'none').sum())\n",
    "\n",
    "# fig, (ax_a, ax_b) = plt.subplots(ncols=2, figsize=(15, 5), width_ratios=[0.3, 0.7])\n",
    "\n",
    "# counts, bins, _ = ax_a.hist(ncbi_genome_metadata_df.gc_percent, bins=25, width=2, edgecolor='black', color='lightgray')\n",
    "# ax_a.set_ylabel('density')\n",
    "# ax_a.set_xlabel('GC percent')\n",
    "\n",
    "# level = 'phylum'\n",
    "# # x = np.arange(genome_metadata_df[f'{level}_taxid'].nunique())\n",
    "# y = ncbi_genome_metadata_df.groupby(level).gc_percent.mean() # .sort_values(ascending=True)\n",
    "# x = ncbi_genome_metadata_df.groupby(level).apply(len, include_groups=False) # .sort_values(ascending=True)\n",
    "# y_err = ncbi_genome_metadata_df.groupby(level).apply(lambda df : df.gc_percent.std() / np.sqrt(len(df)), include_groups=False) # .loc[y.index]\n",
    "\n",
    "# ax_b.errorbar(x, y, yerr=y_err, ls='', lw=0.7, capsize=2, color='black')\n",
    "# ax_b.scatter(x, y, color='black', s=5)\n",
    "# ax_b.set_xscale('log')\n",
    "# ax_b.set_xlabel(f'log({level} size)')\n",
    "# ax_b.set_ylabel('GC percent')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-GC content phylum: Campylobacterota\n",
      "Medium-GC content phylum: Planctomycetota\n",
      "High-GC content phylum: Deinococcota\n"
     ]
    }
   ],
   "source": [
    "# How should I select the genomes for model training?\n",
    "\n",
    "# Using the same genomes for validation as in the Prodigal paper\n",
    "# Halobacterium salinarum\n",
    "# Natronomonas pharaonis\n",
    "# Aeropyrum pernix\n",
    "model_organism_genome_ids = ['GCF_000005845.2'] # E. coli K-12\n",
    "model_organism_genome_ids += ['GCF_000009045.1'] # B. subtilis\n",
    "model_organism_genome_ids += ['GCF_000006765.1'] # P. aeruginosa\n",
    "\n",
    "# Start by selecting a few phyla in a reasonable size range with varying GC contents.\n",
    "def sample(genome_metadata_df:pd.DataFrame, min_phylum_size:int=90, max_phylum_size:int=200):\n",
    "\n",
    "    phylum_sizes = genome_metadata_df.groupby('phylum').apply(len, include_groups=False)\n",
    "    phylum_sizes = phylum_sizes[(phylum_sizes > min_phylum_size) & (phylum_sizes < max_phylum_size)]\n",
    "\n",
    "    genome_metadata_df = genome_metadata_df[genome_metadata_df.phylum.isin(phylum_sizes.index)]\n",
    "    \n",
    "    phylum_gc_percent = genome_metadata_df.groupby('phylum').gc_percent.mean() \n",
    "    phylum_gc_percent = phylum_gc_percent.sort_values(ascending=True)\n",
    "    \n",
    "    phyla = dict()\n",
    "    phyla['low_gc_content'] = phylum_gc_percent.index[0]\n",
    "    phyla['med_gc_content'] = phylum_gc_percent.index[len(phylum_gc_percent) // 2]\n",
    "    phyla['high_gc_content'] = phylum_gc_percent.index[-1]\n",
    "\n",
    "    print('Low-GC content phylum:', phyla['low_gc_content'])\n",
    "    print('Medium-GC content phylum:', phyla['med_gc_content'])\n",
    "    print('High-GC content phylum:', phyla['high_gc_content'])\n",
    "\n",
    "    genome_ids = {category:genome_metadata_df.index[genome_metadata_df.phylum == phylum] for category, phylum in phyla.items()}\n",
    "    return genome_ids\n",
    "\n",
    "genome_ids = sample(ncbi_genome_metadata_df)\n",
    "\n",
    "# ncbi = NCBIDatasets(genome_dir='../data/genomes', gbff_dir='../data/proteins/ncbi')\n",
    "# ncbi.run(genome_ids=genome_ids['low_gc_content'], metadata_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_metadata_df = list()\n",
    "for category, genome_ids_ in genome_ids.items():\n",
    "    df = ncbi_genome_metadata_df.loc[genome_ids_, :].copy()\n",
    "    df['gc_percent_category'] = category\n",
    "    # df = df[genome_metadata_df.annotation_pipeline.str.contains('PGAP')] # Only include genomes annotated with PGAP, because the other files are not consistent. \n",
    "    genome_metadata_df.append(df)\n",
    "genome_metadata_df = pd.concat(genome_metadata_df)\n",
    "genome_metadata_df.to_csv('../data/genome_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in glob.glob('../data/ref/*'):\n",
    "#     genome_id = get_genome_id(path)\n",
    "#     if genome_id not in genome_ids['low_gc_content']\n",
    "#         os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.tools import download_homologs\n",
    "# for path in glob.glob('../data/proteins/ncbi/*'):\n",
    "#     download_homologs(path, pseudo_only=True, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2427.26it/s]\n"
     ]
    }
   ],
   "source": [
    "ncbi = NCBIDatasets()\n",
    "ncbi.run(genome_ids=['GCF_003711085.1'], metadata=False)\n",
    "ncbi.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_hit_genome_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>GCF_003711085.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2703 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     top_hit_genome_id\n",
       "0                 none\n",
       "1      GCF_003711085.1\n",
       "2      GCF_003711085.1\n",
       "3      GCF_003711085.1\n",
       "4      GCF_003711085.1\n",
       "...                ...\n",
       "2698   GCF_003711085.1\n",
       "2699   GCF_003711085.1\n",
       "2700   GCF_003711085.1\n",
       "2701   GCF_003711085.1\n",
       "2702   GCF_003711085.1\n",
       "\n",
       "[2703 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/ref/GCF_003711085.1_summary.csv')[['top_hit_genome_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref: Searching reference for GCF_003711085.1.: 100%|█| 1/1 [00:32<00:00, 32.96s/\n",
      "ref: Search complete. Results written to ../data/ref\n"
     ]
    }
   ],
   "source": [
    "# for genome_id in genome_ids['low_gc_content']:\n",
    "#     ! ref --input-path ../data/proteins/prodigal/{genome_id}_protein.faa --output-dir ../data/ref --gbffs-dir ../data/gbff/ --prodigal-output --summarize\n",
    "! ref --input-path ../data/proteins/prodigal/GCF_003711085.1_protein.faa --overwrite --output-dir ../data/ref --gbffs-dir ../data/ncbi/gbffs/ --prodigal-output --summarize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = load_ref(genome_ids=[genome_id for genome_id in genome_ids['low_gc_content'] if (genome_id != 'GCF_000009085.1')])\n",
    "\n",
    "# My approach to spurious sequences still appears to not be catching everything. \n",
    "# Is this possibly related to the fact that the translate function isn't working properly. \n",
    "\n",
    "# Translate function is OK, but frameshifts are causing problems. Example case is the sequence NC_008229.1_452, which is one of several\n",
    "# sequences in the bounds of a pseudogene with lots of internal stops. Basically, Prodigal has picked up on like 5 proteins, all of which are\n",
    "# sections of the frameshifted pseudogene. However, trying to translate the pseudogene doesn't result in anything that really makes sense, (it just \n",
    "# gets a few amino acids before the internal stop, and then there are a bunch more internal stops). Also, none of the sub-sequences are in-frame\n",
    "# with the labeled pseudogene (this gene doesn't even have a lenght divisible by three). So, all of the Prodigal proteins were registered as \n",
    "# spurious, but all bear homology to the pseudogene. I think I will need to handle validating pseudogenes separately. \n",
    "\n",
    "# I should also note that pseudogenes that are partial in the middle are not necessarily marked partial by their coordinates. Additionally, some\n",
    "# genes marked partial by their coordinates are not pseudo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_hit_genome_id GCF_000259275.1 391\n",
      "query_strand -1 top_hit_strand -1\n",
      "query_seq (402789, 403010) MDFGQKWLGVSVSQNYKNAIKPLFETLEKAKKEGMLWRNFSNKEQEIYAPLLQAFKDEVLRIDENKKNKVPQK\n",
      "top_hit_seq (402451, 403402, codon_start=1) LSAKSNNQGRAYEFAWLLVLEQKLSVFKKVIVDKQGGYNASCRAYESLEKSLQEHYLASAKSGVLLLLDCEPLLSEILDSSHNEIKLSLQTDKLGEIADVRDILIHFDTFCIGLSINIIMRL\n",
      "overlap_length (402789, 403010) 222\n",
      "query_overlap_fraction 1.0 top_hit_overlap_fraction 0.2331932773109243\n",
      "top_hit_product HaeIII family restriction endonuclease\n",
      "n_hits 1\n",
      "top_hit_note frameshifted; Derived by automated computational analysis using gene prediction method: Protein Homology.\n"
     ]
    }
   ],
   "source": [
    "def info(ref_df:pd.DataFrame, id_:str):\n",
    "    row = ref_df.loc[id_]\n",
    "    print('top_hit_genome_id', row.top_hit_genome_id, f'{row.top_hit_id}')\n",
    "    print('query_strand', row.query_strand, 'top_hit_strand', row.top_hit_strand)\n",
    "    print('query_seq', f'({row.query_start}, {row.query_stop})', row.query_seq)\n",
    "    print('top_hit_seq', f'({row.top_hit_start}, {row.top_hit_stop}, codon_start={row.top_hit_codon_start})', row.top_hit_seq)\n",
    "    print('overlap_length', f'({row.overlap_start}, {row.overlap_stop})', row.overlap_length)\n",
    "    print('query_overlap_fraction', row.query_overlap_fraction, 'top_hit_overlap_fraction', row.top_hit_overlap_fraction)\n",
    "    print('top_hit_product', row.top_hit_product)\n",
    "    print('n_hits', row.n_hits)\n",
    "    print('top_hit_note', row.top_hit_note)\n",
    "\n",
    "\n",
    "info(ref_df, 'NC_017735.1_382')\n",
    "# 443855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_datasets: Loading data from 196 genomes belonging to the phylum Campylobacterota.\n",
      "Removing 383 sequences exceeding the maximum length of 2000\n",
      "build_datasets: Loaded 357263 sequences, 297 labeled spurious and 356966 labeled real.\n",
      "MMseqs.load: Removing 300625 non-cluster representatives.\n",
      "build_datasets: Clustering at 50 percent similarity removed 300625 sequences.\n",
      "build_datasets: 240 negative instances and 43785 positive instances in the training dataset.\n",
      "build_datasets: 57 negative instances and 12556 positive instances in the testing dataset.\n"
     ]
    }
   ],
   "source": [
    "def build_datasets(genome_metadata_df:pd.DataFrame, phylum:str='Campylobacterota', max_length:int=2000):\n",
    "    # First remove things which do not have definitive labels. \n",
    "    genome_ids = genome_metadata_df[genome_metadata_df.phylum == phylum].index\n",
    "    genome_ids = [genome_id for genome_id in genome_ids if (genome_id != 'GCF_000009085.1')] # Remove the non-PGAP genome. \n",
    "\n",
    "    print(f'build_datasets: Loading data from {len(genome_ids)} genomes belonging to the phylum {phylum}.')\n",
    "    df = load_ref(genome_ids=genome_ids, add_labels=True)\n",
    "    df = df.rename(columns={'query_seq':'seq'}) # Need to do this for file writing, etc. to work correctly, \n",
    "    df = df.drop(columns=['top_hit_homolog_id', 'top_hit_homolog_seq', 'pseudo'])\n",
    "\n",
    "    lengths = df.seq.apply(len)\n",
    "    print(f'Removing {(lengths >= max_length).sum()} sequences exceeding the maximum length of {max_length}')\n",
    "    df = df[lengths < max_length]\n",
    "    all_df = df.copy()\n",
    "\n",
    "    df = df[df.label != 'none'].copy()\n",
    "    df.label = [0 if (label == 'spurious') else 1 for label in df.label]\n",
    "\n",
    "    print(f'build_datasets: Loaded {len(df)} sequences, {(df.label == 0).sum()} labeled spurious and {(df.label == 1).sum()} labeled real.')\n",
    "\n",
    "    real_df, spurious_df = df[df.label == 1].copy(), df[df.label == 0].copy()\n",
    "    n_real = len(real_df)\n",
    "\n",
    "    mmseqs = MMseqs()\n",
    "    real_df = mmseqs.cluster(real_df, job_name=f'{phylum.lower()}_real_50', sequence_identity=0.50, reps_only=True, overwrite=False)\n",
    "    print(f'build_datasets: Clustering at 50 percent similarity removed {n_real - len(real_df)} sequences.')\n",
    "    mmseqs.cleanup()\n",
    "\n",
    "    df = pd.concat([spurious_df, real_df], ignore_index=False)\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idxs, test_idxs = list(gss.split(df, groups=df.genome_id))[0]\n",
    "    train_df, test_df = df.iloc[train_idxs], df.iloc[test_idxs]\n",
    "    print(f'build_datasets: {(train_df.label == 0).sum()} negative instances and {(train_df.label == 1).sum()} positive instances in the training dataset.')\n",
    "    print(f'build_datasets: {(test_df.label == 0).sum()} negative instances and {(test_df.label == 1).sum()} positive instances in the testing dataset.')\n",
    "\n",
    "    all_df['in_test_dataset'] = all_df.index.isin(test_df.index)\n",
    "    all_df['in_train_dataset'] = all_df.index.isin(train_df.index)\n",
    "    \n",
    "    train_df.to_csv(f'../data/{phylum.lower()}_dataset_train.csv')\n",
    "    test_df.to_csv(f'../data/{phylum.lower()}_dataset_test.csv')\n",
    "    all_df.to_csv(f'../data/{phylum.lower()}_dataset.csv')\n",
    "\n",
    "    return train_df, test_df, all_df \n",
    "    \n",
    "train_df, test_df, all_df = build_datasets(genome_metadata_df)\n",
    "# phylum ='Campylobacterota'\n",
    "# dtypes = {'top_hit_partial':str, 'query_partial':str, 'top_hit_translation_table':str, 'top_hit_codon_start':str}\n",
    "# train_df = pd.read_csv(f'../data/{phylum.lower()}_dataset_train.csv', index_col=0, dtype=dtypes)\n",
    "# test_df = pd.read_csv(f'../data/{phylum.lower()}_dataset_test.csv', index_col=0, dtype=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = load_predict('../data/predict/campylobacterota_dataset_test_predict.csv', model_name='campylobacterota_esm_650m_gap_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.8372044119537454\n",
      "Precision (class 0): 0.0004276702662247407\n",
      "Recall (class 0): 0.07017543859649122\n",
      "Number of false negatives: 3207\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfusion_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m confusion_matrix\n\u001b[1;32m     45\u001b[0m benchmark_fn \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mconfusion_matrix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse negative\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m---> 46\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[43mpred_df\u001b[49m[pred_df\u001b[38;5;241m.\u001b[39mconfusion_matrix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse negative\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m     47\u001b[0m np\u001b[38;5;241m.\u001b[39mintersect1d(fn, benchmark_fn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class BenchmarkClassifier():\n",
    "\n",
    "    def __init__(self, feature_col:str='query_length'):\n",
    "\n",
    "        self.model = LogisticRegression(class_weight='balanced', C=1) # , multi_class='multinomial') \n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "    def predict(self, df:pd.DataFrame, include_outputs:bool=True):\n",
    "        X = df[[self.feature_col]].values\n",
    "        X = self.scaler.transform(X)\n",
    "        model_labels = self.model.predict(X)\n",
    "        model_outputs = self.model.predict_proba(X)\n",
    "\n",
    "        return model_labels if (not include_outputs) else (model_labels, model_outputs)\n",
    "\n",
    "    def fit(self, df:pd.DataFrame):\n",
    "        X, labels = df[[self.feature_col]].values, df['label'].values\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X, labels)\n",
    "\n",
    "model = BenchmarkClassifier()\n",
    "model.fit(train_df)\n",
    "\n",
    "df = test_df[['label', 'query_length']].copy()\n",
    "df['model_label'], outputs = model.predict(test_df, include_outputs=True)\n",
    "df['model_output_0'], df['model_output_1'] = outputs[:, 0].ravel(), outputs[:, 1].ravel()\n",
    "\n",
    "print('Balanced accuracy:', balanced_accuracy_score(df.label.values, df.model_label.values))\n",
    "print('Precision (class 0):', precision(df, class_=0))\n",
    "print('Recall (class 0):', recall(df, class_=0))\n",
    "print('Number of false negatives:', ((df.label == 1) & (df.model_label == 0)).sum())\n",
    "\n",
    "\n",
    "confusion_matrix = np.where((df.model_label == 1) & (df.label == 0), 'false positive', '')\n",
    "confusion_matrix = np.where((df.model_label  == 1) & (df.label == 1), 'true positive', confusion_matrix)\n",
    "confusion_matrix = np.where((df.model_label == 0) & (df.label == 1), 'false negative', confusion_matrix)\n",
    "confusion_matrix = np.where((df.model_label  == 0) & (df.label == 0), 'true negative', confusion_matrix)\n",
    "df['confusion_matrix'] = confusion_matrix\n",
    "\n",
    "benchmark_fn = df[df.confusion_matrix == 'false negative'].index\n",
    "fn = pred_df[pred_df.confusion_matrix == 'false negative'].index\n",
    "np.intersect1d(fn, benchmark_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There still seems to be something wrong with the training data, perhaps I am still misclassifying the spurious sequences?\n",
    "# It is possible my \"in-frame\" check is wrong, it's possible I should only see if either edge is a multiple of 3 base pairs away, \n",
    "# as opposed to both.\n",
    "\n",
    "# It seems as though 684 of the sequences flagged as spurious have an overlap length of 0, so maybe I should be counting those as intergenic?\n",
    "# Ok, so I think because of how frequent gene overlap is, I should be allowing some (considering genes with less than a certain amount of overlap to be intergenic)\n",
    "# I should also be very conservative, i.e. allow a comparatively large amount of overlap. \n",
    "# https://pmc.ncbi.nlm.nih.gov/articles/PMC525685/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Curious about the distribution of overlap sizes... \n",
    "\n",
    "\n",
    "# def figure(ref_df:pd.DataFrame, path:str=None, min_overlap_fraction:float=0.8):\n",
    "#     # ref_df = load_ref(genome_ids=genome_ids)\n",
    "#     ref_df = ref_df[ref_df.spurious | ref_df.real].copy()\n",
    "\n",
    "#     fig = plt.figure(figsize=(10, 10), layout='tight')\n",
    "#     grid = GridSpec(nrows=1, ncols=1, figure=fig)\n",
    "#     ax_a = fig.add_subplot(grid[0, 0])\n",
    "\n",
    "#     ax_a_df = pd.DataFrame(index=ref_df.index)\n",
    "#     ax_a_df['length'] = get_lengths(ref_df, top_hit=False, units='nt')\n",
    "#     ax_a_df['overlap_length'] = ref_df['top_hit_overlap_length']\n",
    "#     ax_a_df['overlap_fraction'] = ax_a_df.overlap_length / ax_a_df.length # This is relative to the query sequence. \n",
    "#     ax_a_df['label'] = np.select([ref_df.real, ref_df.spurious], ['real', 'spurious'], default='none')\n",
    "#     if min_overlap_fraction is not None:\n",
    "#         ax_a_df = ax_a_df[ax_a_df.overlap_fraction > min_overlap_fraction].copy()\n",
    "\n",
    "#     colors = ['darkseagreen', 'indianred']\n",
    "#     # sns.kdeplot(data=ax_a_df, ax=ax_a, hue='label', hue_order=['real', 'spurious'], palette=colors, common_norm=False, x='overlap_fraction')\n",
    "#     sns.ecdfplot(data=ax_a_df, ax=ax_a, hue='label', hue_order=['real', 'spurious'], palette=colors, x='overlap_fraction')\n",
    "\n",
    "#     plt.show()\n",
    "#     return ax_a_df\n",
    "\n",
    "\n",
    "# ax_a_df = figure(ref_df, min_overlap_fraction=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tripy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
