{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from Bio import SeqIO\n",
    "import src.tools.download\n",
    "from src.tools import MMSeqs\n",
    "from src.dataset import Dataset\n",
    "from src.files import FASTAFile, GBFFFile\n",
    "from src.reference import Reference\n",
    "from src.clusterer import Clusterer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json \n",
    "import seaborn as sns\n",
    "from src import fillna \n",
    "import matplotlib.pyplot as plt \n",
    "from src.split import ClusterStratifiedShuffleSplit\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Because genomes are constantly being re-annotated, the GBFF files I downloaded a month or so ago are now not aligned with the \n",
    "# protein files I used to generate the most recent round of embeddings (05/23/2025). GBFF files were re-downloaded for \n",
    "# the Campylobacterota phylum on 05/25/2025. \n",
    "\n",
    "# ncbi = src.tools.download.NCBI()\n",
    "# ncbi.get_genomes(genome_ids=genome_ids, include=['gbff'], dirs={'gbff':'../data/ncbi/gbffs/'})\n",
    "# ncbi.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing my training dataset strategy. Instead going to use the sequences directly from NCBI. \n",
    "genome_metadata_df = pd.read_csv('../data/genome_metadata.csv', index_col=0)\n",
    "genome_ids = genome_metadata_df[genome_metadata_df.phylum == 'Campylobacterota'].index.values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Campylobacterota dataset.: 100%|██████████| 197/197 [06:16<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 44475 suspect sequences from the Campylobacterota dataset.\n",
      "Removing 341 sequences exceeding the maximum length from the Campylobacterota dataset.\n"
     ]
    }
   ],
   "source": [
    "# Want to build the dataset for Campylobacterota, but filter out the \"suspect\" sequences, i.e. those which are both hypothetical\n",
    "# and have only ab initio evidence (i.e. no evidence of conservation). \n",
    "\n",
    "is_hypothetical = lambda df : df['product'] == 'hypothetical protein'\n",
    "is_ab_initio = lambda df : df.evidence_type == 'ab initio prediction'\n",
    "is_suspect = lambda df : is_hypothetical(df) & is_ab_initio(df) # This will be False for intergenic sequences. \n",
    "\n",
    "ncbi_df = list()\n",
    "for genome_id in tqdm(genome_ids, desc='Building Campylobacterota dataset.'):\n",
    "    protein_path = f'../data/ncbi/proteins/{genome_id}_protein.faa'\n",
    "    gbff_path = f'../data/ncbi/gbffs/{genome_id}_genomic.gbff'\n",
    "\n",
    "    gbff_df = GBFFFile(gbff_path).to_df()\n",
    "    gbff_df = gbff_df[(gbff_df.feature == 'CDS') & (~gbff_df.pseudo)].copy()\n",
    "    copy_numbers = gbff_df.protein_id.value_counts() # There are multiple copies of the same protein at different coordinates in the GBFF files with different coordinates. \n",
    "    gbff_df['copy_number'] = gbff_df.protein_id.map(copy_numbers)\n",
    "    gbff_df = gbff_df.drop_duplicates('protein_id').copy()\n",
    "    gbff_df = gbff_df.drop(columns=['seq']) # Use the sequences from the protein DataFrame, just to make sure everything is equal. \n",
    "    gbff_df = gbff_df.set_index('protein_id')\n",
    "    gbff_df.index.name = 'id'\n",
    "\n",
    "    protein_df = FASTAFile(path=protein_path).to_df(prodigal_output=False)\n",
    "    protein_df = protein_df.drop(columns=['description'])\n",
    "\n",
    "    assert len(protein_df) == len(gbff_df), 'Expected the number of non-pseudo CDS entries in the GBFF file to match the entries in the FASTA file.'\n",
    "    assert np.all(np.sort(protein_df.index) == np.sort(gbff_df.index)), 'Expected the number of non-pseudo CDS entries in the GBFF file to match the entries in the FASTA file.'\n",
    "    assert protein_df.index.is_unique and gbff_df.index.is_unique, 'Expected the indices of both DataFrames to be unique.'\n",
    "\n",
    "    ncbi_df.append(protein_df.merge(gbff_df, left_index=True, right_index=True).assign(genome_id=genome_id))\n",
    "\n",
    "ncbi_df = pd.concat(ncbi_df)\n",
    "\n",
    "mask = is_suspect(ncbi_df)\n",
    "print(f'Removing {mask.sum()} suspect sequences from the Campylobacterota dataset.')\n",
    "ncbi_df = ncbi_df[~mask].copy()\n",
    "\n",
    "ncbi_df.to_csv('../data/campylobacterota.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 9789 non-bacterial sequences from the AntiFam dataset.\n"
     ]
    }
   ],
   "source": [
    "antifam_df = pd.read_csv('../data/antifam.csv', index_col=0)\n",
    "ncbi_df = pd.read_csv('../data/campylobacterota.csv', index_col=0)\n",
    "\n",
    "is_bacterial = lambda df : np.array(['2' in lineage.split(' ') for lineage in df.lineage])\n",
    "\n",
    "mask = ~is_bacterial(antifam_df)\n",
    "print(f'Removing {mask.sum()} non-bacterial sequences from the AntiFam dataset.')\n",
    "antifam_df = antifam_df[~mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 16 sequences exceeding the maximum length from the Campylobacterota dataset.\n",
      "Removing 9522 duplicate sequences from the Campylobacterota dataset\n"
     ]
    }
   ],
   "source": [
    "ncbi_df['library_entry_name'] = ncbi_df.genome_id \n",
    "ncbi_df['label'] = 1\n",
    "\n",
    "antifam_df['library_entry_name'] = 'antifam'\n",
    "antifam_df['label'] = 0\n",
    "\n",
    "dataset_df = pd.concat([antifam_df, ncbi_df])\n",
    "dataset_df = fillna(dataset_df, rules={str:'none', bool:False, int:0, float:0})\n",
    "\n",
    "mask = (dataset_df.seq.apply(len) >= 2000) # Upper length bound is non-inclusive. \n",
    "print(f'Removing {mask.sum()} sequences exceeding the maximum length from the Campylobacterota dataset.')\n",
    "dataset_df = dataset_df[~mask].copy()\n",
    "\n",
    "def check_duplicate_ids_have_identical_sequences(dataset_df):\n",
    "    duplicate_ids = dataset_df.index[dataset_df.index.duplicated()].unique()\n",
    "    for id_ in tqdm(duplicate_ids, desc='check_duplicate_ids_have_identical_sequences'):\n",
    "        seqs = dataset_df.seq[dataset_df.index == id_]\n",
    "        assert np.all(seqs == seqs.iloc[0]), f'check_duplicate_ids_have_identical_sequences: Sequences with ID {id_} are not equal.'\n",
    "\n",
    "# check_duplicate_ids_have_identical_sequences(dataset_df)\n",
    "\n",
    "mask = dataset_df.seq.duplicated(keep='first')\n",
    "print(f'Removing {mask.sum()} duplicate sequences from the Campylobacterota dataset')\n",
    "dataset_df = dataset_df[~mask].copy()\n",
    "\n",
    "dataset_df.to_csv('../data/dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subset_df = dataset_df.sample(5000, random_state=42)\n",
    "dataset_subset_df.to_csv('../data/dataset_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a good estimate for the number of clusters? Maybe use 50 percent sequence similarity?\n",
    "mmseqs = MMSeqs()\n",
    "mmseqs_cluster_df = mmseqs.cluster(dataset_df, job_name='cluster', output_dir='../data', sequence_identity=0.5, overwrite=False)\n",
    "mmseqs.cleanup()\n",
    "print('Number of clusters:', dataset_df.cluster_label.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterStratifiedShuffleSplit._split_non_homogenous_clusters: Found 538 non-homogenous clusters.\n",
      "ClusterStratifiedShuffleSplit._load_clusters: Found 2001 singleton clusters with \"real\" labels.\n",
      "ClusterStratifiedShuffleSplit._load_clusters: Found 422 singleton clusters with \"spurious\" labels.\n",
      "ClusterStratifiedShuffleSplit.__init__: Adjusted training and test sizes are 0.801, 0.199.\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(dataset_path:str='../data/dataset_dereplicated.csv', cluster_path:str='../data/dataset_dereplicated_cluster.csv'):\n",
    "    dataset = Dataset.from_csv(dataset_path, feature_type=None)\n",
    "    splits = ClusterStratifiedShuffleSplit(dataset, cluster_path=cluster_path, n_splits=1)\n",
    "    dataset_train, dataset_test = list(splits)[0]\n",
    "    dataset_train.to_csv('../data/dataset_train.csv', metadata=True)\n",
    "    dataset_test.to_csv('../data/dataset_test.csv', metadata=True)\n",
    "\n",
    "    return dataset_train.to_df(metadata=True), dataset_test.to_df(metadata=True)\n",
    "\n",
    "# train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodigal_df = []\n",
    "for genome_id in genome_ids:\n",
    "    ref_df = Reference.load_ref(f'../data/ref/{genome_id}_ref.csv')\n",
    "    ref_df['genome_id'] = genome_id\n",
    "    prodigal_df.append(ref_df)\n",
    "prodigal_df = pd.concat(prodigal_df)\n",
    "prodigal_df.to_csv('../data/prodigal.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cluster_info(dataset_df:pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "#     df = list()\n",
    "#     for cluster_label, cluster_df in dataset_df.groupby('cluster_label'):\n",
    "#         cluster_dists = cluster_df.distance_to_cluster_center\n",
    "#         row = dict()\n",
    "#         row['cluster_label'] = cluster_label\n",
    "#         row['max_distance_to_cluster_center'] = cluster_dists.max()\n",
    "#         row['mean_distance_to_cluster_center'] = cluster_dists.mean()\n",
    "#         row['min_distance_to_cluster_center'] = cluster_dists.min()\n",
    "#         row['size'] = len(cluster_df)\n",
    "#         row['homogenous'] = (cluster_df.label.nunique() == 1)\n",
    "\n",
    "#         n_real, n_spurious = (cluster_df.label == 1).sum(), (cluster_df.label == 0).sum()\n",
    "#         row['n_spurious'] = n_spurious\n",
    "#         row['n_real'] = n_real\n",
    "#         # row['majority_label'] = np.nan if (n_real == n_spurious) else int(n_real > n_spurious)\n",
    "#         row['majority_label'] = int(n_real >= n_spurious)\n",
    "\n",
    "#         if not row['homogenous']:\n",
    "#             majority_label = row['majority_label']\n",
    "#             minority_label = int(not majority_label)\n",
    "#             row['mean_distance_to_cluster_center_majority_label'] = cluster_df.distance_to_cluster_center[cluster_df.label == majority_label].mean()\n",
    "#             row['mean_distance_to_cluster_center_minority_label'] = cluster_df.distance_to_cluster_center[cluster_df.label == minority_label].mean()        \n",
    "#         df.append(row)\n",
    "#     df = pd.DataFrame(df).set_index('cluster_label')\n",
    "#     return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frugal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
