{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "trailing comma not allowed without surrounding parentheses (2884041058.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    from sklearn.metrics import balanced_accuracy_score, confusion_matrix,\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m trailing comma not allowed without surrounding parentheses\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from utils import * \n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, \n",
    "from src.graph import NeighborsGraph \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "def save_table(df:pd.DataFrame, path:str=None):\n",
    "    if path is None:\n",
    "        return \n",
    "    elif path.split('.')[-1] == 'csv':\n",
    "        df.to_csv(path)\n",
    "    elif path.split('.')[-1] == 'png':\n",
    "        dfi.export(df, path)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(df:pd.DataFrame, label:int=0, threshold:float=0.95) -> float:\n",
    "    '''Computes the precision of the model's predictions. Recall summarizes how well the positive class was predicted and is the same calculation as sensitivity.'''\n",
    "    model_labels = np.where(df[f'model_output_{label}'] > threshold, label, int(not label))\n",
    "    n = ((model_labels == label) & (df.label == label)).sum()\n",
    "    N = (df.label == label).sum() # Total number of relevant instances (i.e. members of the class)\n",
    "    return n / N\n",
    "\n",
    "def precision(df:pd.DataFrame, label:int=0, threshold:float=0.95) -> float:\n",
    "    '''Computes the precision of the model's predictions. Precision summarizes the fraction of examples assigned the positive class that belong to the positive class.'''\n",
    "    model_labels = np.where(df[f'model_output_{label}'] > threshold, label, int(not label))\n",
    "    n = ((model_labels == label) & (df.label == label)).sum()\n",
    "    N = (model_labels == label).sum() # Total number of retrieved instances (i.e. predicted members of the class)\n",
    "    return n / N\n",
    "\n",
    "def balanced_accuracy(df:pd.DataFrame):\n",
    "    '''Compute the balanced accuracy.'''\n",
    "    return balanced_accuracy_score(df.label, df.model_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.read_csv('../data/results/dataset_test_predict.csv', index_col=0).assign(dataset='test')\n",
    "test_results_df = test_results_df.merge(pd.read_csv('../data/datasets/dataset_test.csv', index_col=0), left_index=True, right_index=True, how='left')\n",
    "\n",
    "train_results_df = pd.read_csv('../data/results/dataset_train_predict.csv', index_col=0).assign(dataset='train')\n",
    "train_results_df = train_results_df.merge(pd.read_csv('../data/datasets/dataset_train.csv', index_col=0), left_index=True, right_index=True, how='left')\n",
    "\n",
    "results_df = pd.concat([train_results_df, test_results_df])\n",
    "results_df['length'] = results_df.seq.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_positive</th>\n",
       "      <th>true_negative</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>false_negative</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_v1</th>\n",
       "      <td>258325</td>\n",
       "      <td>17221</td>\n",
       "      <td>3</td>\n",
       "      <td>1041</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_v2</th>\n",
       "      <td>258687</td>\n",
       "      <td>17216</td>\n",
       "      <td>8</td>\n",
       "      <td>679</td>\n",
       "      <td>0.9985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_v3</th>\n",
       "      <td>258686</td>\n",
       "      <td>17214</td>\n",
       "      <td>10</td>\n",
       "      <td>680</td>\n",
       "      <td>0.9984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            true_positive  true_negative  false_positive  false_negative  \\\n",
       "model_name                                                                 \n",
       "model_v1           258325          17221               3            1041   \n",
       "model_v2           258687          17216               8             679   \n",
       "model_v3           258686          17214              10             680   \n",
       "\n",
       "            accuracy  \n",
       "model_name            \n",
       "model_v1      0.9979  \n",
       "model_v2      0.9985  \n",
       "model_v3      0.9984  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def table_1(results_df:pd.DataFrame):\n",
    "\n",
    "    model_names = [os.path.basename(path).replace('.pkl', '') for path in glob.glob('../models/*pkl')]\n",
    "\n",
    "    table_df = list()\n",
    "    for model_name in model_names:\n",
    "        row = dict()\n",
    "        row['model_name'] = model_name\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(results_df['label'], results_df[f'{model_name}_label']).ravel()\n",
    "        row['true_positive'] = tp\n",
    "        row['true_negative'] = tn\n",
    "        row['false_positive'] = fp\n",
    "        row['false_negative'] = fn\n",
    "        row['accuracy'] = np.round(balanced_accuracy_score(results_df.label, results_df[f'{model_name}_label']), 4)\n",
    "        table_df.append(row)\n",
    "\n",
    "    table_df = pd.DataFrame(table_df).set_index('model_name')\n",
    "    table_df = table_df.sort_values('false_positive')\n",
    "\n",
    "    # dfi.export(table_df, path, table_conversion='matplotlib')\n",
    "    return table_df\n",
    "\n",
    "table_1(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_v2'\n",
    "train_results_df = train_results_df.rename(columns={col:col.replace(model_name, 'model') for col in train_results_df.columns})\n",
    "test_results_df = test_results_df.rename(columns={col:col.replace(model_name, 'model') for col in test_results_df.columns})\n",
    "results_df = results_df.rename(columns={col:col.replace(model_name, 'model') for col in results_df.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# figure(results_df, threshold=0.7, label=1)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m figure(\u001b[43mresults_df\u001b[49m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "def figure(results_df, threshold:float=0.95, label:int=1):\n",
    "\n",
    "    min_threshold = results_df[f'model_output_{label}'].min() + 1e-3\n",
    "    max_threshold = results_df[f'model_output_{label}'].max() - 1e-3\n",
    "\n",
    "    thresholds = [min_threshold] + list(np.linspace(0.1, 0.99, 20)) + [max_threshold]\n",
    "    ax_df = pd.DataFrame(index=np.arange(len(thresholds)))\n",
    "    ax_df['threshold'] = thresholds\n",
    "    ax_df['precision'] = [precision(results_df, label=label, threshold=threshold) for threshold in thresholds]\n",
    "    ax_df['recall'] = [recall(results_df, label=label, threshold=threshold) for threshold in thresholds]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    sns.lineplot(data=ax_df, ax=ax, x='threshold', y='precision', label='precision', color='lightgray')\n",
    "    sns.lineplot(data=ax_df, ax=ax, x='threshold', y='recall', label='recall', color='gray')\n",
    "    ax.axvline(threshold, color='black', linewidth=0.5, linestyle='--')\n",
    "\n",
    "    results_df['model_label'] = np.where(results_df[f'model_output_{label}'] > threshold, label, int(not label))\n",
    "    n_fn = ((results_df.model_label == 0) & (results_df.label == 1)).sum()\n",
    "    n_fp = ((results_df.model_label == 1) & (results_df.label == 0)).sum()\n",
    "    ax.text(0.5, 0.5, '$n_{fp} = $' + f'${n_fp}$', transform=ax.transAxes)\n",
    "    ax.text(0.5, 0.45, '$n_{fn} = $' + f'${n_fn}$', transform=ax.transAxes)\n",
    "\n",
    "    ax.set_ylabel('')\n",
    "    x_ticks = sorted(list(np.arange(10) / 10) + [threshold])\n",
    "    ax.set_xticks(x_ticks, labels=x_ticks)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# figure(results_df, threshold=0.7, label=1)\n",
    "figure(results_df, threshold=0.95, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false positives: 280 out of 17224\n",
      "Number of false positives in testing dataset: 67 out of 3268\n",
      "Number of false positives in training dataset: 213 out of 13956\n",
      "\n",
      "Number of false negatives: 300 out of 259366\n",
      "Number of false negatives in testing dataset: 13 out of 51690\n",
      "Number of false negatives in train dataset: 287 out of 207676\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.95\n",
    "results_df['model_label'] = np.where(results_df.model_output_0 > threshold, 0, 1)\n",
    "\n",
    "fp_results_df = results_df[(results_df.model_label == 1) & (results_df.label == 0)].copy()\n",
    "fn_results_df = results_df[(results_df.model_label == 0) & (results_df.label == 1)].copy()\n",
    "tp_results_df = results_df[(results_df.model_label == 1) & (results_df.label == 1)].copy()\n",
    "tn_results_df = results_df[(results_df.model_label == 0) & (results_df.label == 0)].copy()\n",
    "\n",
    "print('Number of false positives:', len(fp_results_df), 'out of', (results_df.label == 0).sum())\n",
    "print('Number of false positives in testing dataset:', (fp_results_df.dataset == 'test').sum(), 'out of', (test_results_df.label == 0).sum())\n",
    "print('Number of false positives in training dataset:', (fp_results_df.dataset == 'train').sum(), 'out of', (train_results_df.label == 0).sum(), end='\\n\\n')\n",
    "\n",
    "print('Number of false negatives:', len(fn_results_df), 'out of', (results_df.label == 1).sum())\n",
    "print('Number of false negatives in testing dataset:', (fn_results_df.dataset == 'test').sum(), 'out of', (test_results_df.label == 1).sum())\n",
    "print('Number of false negatives in train dataset:', (fn_results_df.dataset == 'train').sum(), 'out of', (train_results_df.label == 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy on training dataset: 0.9916778934220855\n",
      "Balanced accuracy on testing dataset: 0.9896233323459007\n"
     ]
    }
   ],
   "source": [
    "print('Balanced accuracy on training dataset:', balanced_accuracy(results_df[results_df.dataset == 'train']))\n",
    "print('Balanced accuracy on testing dataset:', balanced_accuracy(results_df[results_df.dataset == 'test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9999883779941192\n",
      "Precision on training dataset: 0.999990317161379\n",
      "Precision on testing dataset: 0.9999806126405584\n",
      "\n",
      "Recall: 0.9952268223282928\n",
      "Recall on training dataset: 0.9945732776055008\n",
      "Recall on testing dataset: 0.997852582704585\n"
     ]
    }
   ],
   "source": [
    "# Metrics computed with respect to the positive (real) class. \n",
    "\n",
    "print('Precision:', precision(results_df, label=1))\n",
    "print('Precision on training dataset:', precision(results_df[results_df.dataset == 'train'], label=1))\n",
    "print('Precision on testing dataset:', precision(results_df[results_df.dataset == 'test'], label=1), end='\\n\\n')\n",
    "\n",
    "print('Recall:', recall(results_df, label=1))\n",
    "print('Recall on training dataset:', recall(results_df[results_df.dataset == 'train'], label=1))\n",
    "print('Recall on testing dataset:', recall(results_df[results_df.dataset == 'test'], label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9826026443980515\n",
      "Precision on training dataset: 0.979543834640057\n",
      "Precision on testing dataset: 0.9959551960174238\n",
      "\n",
      "Recall: 0.983743613562471\n",
      "Recall on training dataset: 0.984737747205503\n",
      "Recall on testing dataset: 0.9794981640146879\n"
     ]
    }
   ],
   "source": [
    "# Metrics computed with respect to the negative (spurious) class. \n",
    "\n",
    "print('Precision:', precision(results_df, label=0))\n",
    "print('Precision on training dataset:', precision(results_df[results_df.dataset == 'train'], label=0))\n",
    "print('Precision on testing dataset:', precision(results_df[results_df.dataset == 'test'], label=0), end='\\n\\n')\n",
    "\n",
    "print('Recall:', recall(results_df, label=0))\n",
    "print('Recall on training dataset:', recall(results_df[results_df.dataset == 'train'], label=0))\n",
    "print('Recall on testing dataset:', recall(results_df[results_df.dataset == 'test'], label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frugal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
